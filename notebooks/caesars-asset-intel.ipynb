{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import datetime as dt\n",
    "from io import StringIO\n",
    "import json\n",
    "from typing import Any, Dict, Iterable\n",
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import shutil\n",
    "import zstandard as zstd\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from rapidfuzz import fuzz, distance  # well-maintained Levenshtein scoring\n",
    "from jaro import jaro_winkler_metric\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/csvs/data_mappings.csv', 'r') as f:\n",
    "    data_mapping_csv = csv.DictReader(f)\n",
    "    # for row in reader:\n",
    "    #     print(row)  # each row is a list of strings\n",
    "#\n",
    "# with open(\"../data/csvs/data_mappings.csv\", \"r\") as f:\n",
    "#     data_mapping_csv=csv.reader(f.read())\n",
    "data_mapping_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "print(boto3.Session().get_credentials().get_frozen_credentials())\n",
    "\n",
    "s3_okta = [\"s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs_1.zst\",\n",
    "\"s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs_2.zst\",\"s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs.zst\"]\n",
    "def parse_s3_url(s3_url: str) -> tuple[str, str]:\n",
    "    parsed = urlparse(s3_url)\n",
    "    if parsed.scheme != \"s3\" or not parsed.netloc or not parsed.path:\n",
    "        raise ValueError(f\"Invalid S3 URL format: {s3_url}\")\n",
    "    bucket = parsed.netloc\n",
    "    key = parsed.path.lstrip(\"/\")\n",
    "    return bucket, key\n",
    "\n",
    "def download_s3_url_object(s3_url: str, destination_path: str, region: str = \"us-west-1\") -> bool:\n",
    "    try:\n",
    "        bucket, key = parse_s3_url(s3_url)\n",
    "        s3_client = boto3.client(\"s3\", region_name=region)\n",
    "        s3_client.download_file(Bucket=bucket, Key=key, Filename=destination_path)\n",
    "        print(f\"Downloaded '{key}' from '{bucket}' to '{destination_path}'\")\n",
    "        return True\n",
    "    except (ValueError, ClientError, BotoCoreError) as e:\n",
    "        print(f\"[ERROR] {type(e).__name__}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[UNEXPECTED ERROR] {e}\")\n",
    "    return False\n",
    "data_list = []\n",
    "for s in s3_okta:\n",
    "    data_list.append(download_s3_url_object(s, '../data/test/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Load Data:\n",
    "S3 - 48508c3f-af34-4272-913f-331cb2b3db4b\n",
    "- https://us-west-1.console.aws.amazon.com/s3/buckets/reach-sandbox1?region=us-west-1&bucketType=general&prefix=customers%2F48508c3f-af34-4272-913f-331cb2b3db4b%2F&showversions=false\n",
    "\n",
    "*\"...Also found out Caesars (the hotel/casino) runs BloodHound... Their Okta and CrowdStrike are still connected. AD has been disconnected for a while so it'll be a little out of date. Did a POV with them last year and all of Vegas ended up having budget issues, finally coming back to them and Wynn.\" - Colt*\n",
    "\n",
    "We will use the date 2024-11-20 for each data source, crowdstrike data is a non-empty JSON that has only keys all the way back to 2024-10-10.\n",
    "\n",
    "`aws s3 cp s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs.zst ./ --region us-west-1`\n",
    "\n",
    "Okta:\n",
    "- s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs_1.zst\n",
    "- s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs_2.zst\n",
    "- s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/okta/system_logs/2024-11-20/okta_logs.zst\n",
    "\n",
    "TAP:\n",
    "- s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/tap/attacks/2024/11-20/users.zst\n",
    "\n",
    "MS Exchange:\n",
    "- s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/exchange_org_details/2024-11-20.zs\n",
    "\n",
    "Azure AD:\n",
    "- s3://reach-sandbox1/customers/48508c3f-af34-4272-913f-331cb2b3db4b/ad/azure/2024-11-20/azure_ad.zst\n",
    "\n",
    "Customer data source: https://us-west-1.console.aws.amazon.com/s3/buckets/reach-sandbox1?bucketType=general&prefix=customers%2F297ede4a-3dc4-4571-8aed-93f7813fccc1%2F&region=us-west-1&tab=objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress target files\n",
    "def normalize(name: str) -> str:\n",
    "    name = name.lower().strip()\n",
    "    name = re.sub(r'\\s+', '_', name)\n",
    "    name = re.sub(r'[^a-z0-9._-]', '', name)\n",
    "    return name\n",
    "\n",
    "def unzst_directory(src_dir: pathlib.Path):\n",
    "    src_dir = src_dir.resolve()\n",
    "    for root, _, files in os.walk(src_dir):\n",
    "        root = pathlib.Path(root)\n",
    "        rel = root.relative_to(src_dir)\n",
    "        target_root = src_dir / rel\n",
    "        target_root.mkdir(parents=True, exist_ok=True)\n",
    "        for file in files:\n",
    "            if file.endswith('.zst'):\n",
    "                inpath = root / file\n",
    "                stem = pathlib.Path(normalize(file[:-4]))\n",
    "                outpath = target_root / stem\n",
    "                print(f\"Decompressing {inpath} → {outpath}\")\n",
    "                with open(inpath, 'rb') as ifh, open(outpath, 'wb') as ofh:\n",
    "                    dctx = zstd.ZstdDecompressor()\n",
    "                    dctx.copy_stream(ifh, ofh)\n",
    "\n",
    "def dict_with_most_keys(dicts: Iterable[Dict[Any, Any]]) -> Dict[Any, Any]:\n",
    "    \"\"\"\n",
    "    Returns the dictionary from the iterable that has the most keys.\n",
    "    If multiple dictionaries are tied, returns the first one encountered.\n",
    "    Raises ValueError if the iterable is empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # `max` with key=len chooses the dict with the largest number of keys\n",
    "        return max(dicts, key=len)\n",
    "    except ValueError as e:\n",
    "        # This will occur if dicts is empty\n",
    "        raise ValueError(\"The input iterable must contain at least one dictionary\") from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unzst_directory(pathlib.Path(\"/data/customers/caesars\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### Okta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_data = None #System logs\n",
    "okta_data_1 = None #System logs\n",
    "okta_data_2 = None #System logs\n",
    "okta_classic_config_data = None\n",
    "okta_classic_group_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are Okta System Logs:https://developer.okta.com/docs/api/openapi/okta-management/management/tag/SystemLog/\n",
    "with open(\"../data/customers/caesars/okta/2024-11-20_okta_logs_1.json\", 'r') as f:\n",
    "    okta_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(okta_data[0], indent=2))\n",
    "print(len(okta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/customers/caesars/okta/2024-11-20_okta_logs_1.json\", 'r') as f:\n",
    "    okta_data_1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(okta_data_1[0], indent=2))\n",
    "print(len(okta_data_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/customers/caesars/okta/2024-11-20_okta_logs_1.json\", 'r') as f:\n",
    "    okta_data_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(okta_data_2[0], indent=2))\n",
    "print(len(okta_data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/customers/caesars/okta/2024-11-20_classic_okta_configs.json\", 'r') as f:\n",
    "    okta_classic_config_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okta_classic_config_data.keys())\n",
    "print(list(okta_classic_config_data.get(\"devices\").keys())[0:5])\n",
    "print(len(okta_classic_config_data.get(\"devices\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/customers/caesars/okta/2024-11-20_classic_okta_groups.json\", 'r') as f:\n",
    "    okta_classic_group_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okta_classic_group_data.keys())\n",
    "print(list(okta_classic_group_data.get(\"Everyone\").keys())[0:5])\n",
    "print(len(okta_classic_group_data.get(\"Everyone\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine okta system logs\n",
    "okta_system_logs = okta_data + okta_data_1 + okta_data_2\n",
    "print(len(okta_system_logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Join okta log data #1---unsure as to what the difference is yet---on AD USER data\n",
    "Multiple ID fields for Okta JSON log data:\n",
    "    - uuid = is the even ID, generated by okta to identify an event\n",
    "    - actor.id = (initiator ID) a 20-character string beginning with `00u` for the system, or user that triggered the event\n",
    "    - actor.alternateId = can contain username, or email\n",
    "    - target.id = Generated by okta\n",
    "    - client.id = client/application identifier OAuth client, API token, or agent. Okta internal ID\n",
    "    - transaction.id = group events that occur together in the same flow\n",
    "    - externalSessionId = correlates events within the same okta session\n",
    "\n",
    "Name fields:\n",
    "    - displayName\n",
    "    - alternateId\n",
    "\n",
    "The IDs generated by Okta will likely not help correlation of the data, username, IP, email, computer name if not none,\n",
    "\n",
    "Links:\n",
    "- ipChans - https://www.obsidiansecurity.com/blog/how-to-use-client-ip-addresses-in-okta-audit-logs\n",
    "\n",
    "Example Device data from AD:\n",
    "```JSON\n",
    "{\n",
    "    \"displayName\": \"Kendell Douglas\",\n",
    "    \"mailNickname\": \"kdouglas1\",\n",
    "    \"givenName\": \"Kendell\",\n",
    "    \"surname\": \"Douglas\",\n",
    "    \"jobTitle\": \"Retail Planning Manager\",\n",
    "    \"department\": \"CLV_HEC: 08800_CES 9IF_CEI MERCHANDISING ADMIN\",\n",
    "    \"mail\": \"kdouglas1@caesars.com\",\n",
    "    \"otherMails\": [\n",
    "        \"kdouglas1@lvrio.harrahs.com\"\n",
    "    ],\n",
    "...\n",
    "    \"devices\": [\n",
    "       {\n",
    "        \"id\": \"d357df7c-ab45-4f00-a65b-ec9ec751638a\",\n",
    "        \"displayName\": \"LVR053335284353\",\n",
    "        \"operatingSystem\": \"Windows\",\n",
    "        \"operatingSystemVersion\": \"10.0.16299.0\",\n",
    "        \"profileType\": \"RegisteredDevice\",\n",
    "        \"accountEnabled\": true,\n",
    "        \"approximateLastSignInDateTime\": \"2020-03-17T17:51:55Z\",\n",
    "        \"createdDateTime\": \"2019-01-30T19:58:32Z\",\n",
    "        \"deviceId\": \"cd926deb-8e85-4628-850d-34ba6f485856\",\n",
    "        \"deviceVersion\": 2,\n",
    "        \"registrationDateTime\": \"2019-01-30T11:58:32Z\",\n",
    "        \"trustType\": \"Workplace\"\n",
    "        }\n",
    "      ]\n",
    "}\n",
    "```\n",
    "\n",
    "Okta security context:\n",
    "```JSON\n",
    " 'securityContext': {'asNumber': 394089,\n",
    "  'asOrg': 'palo alto networks  inc',\n",
    "  'isp': 'google',\n",
    "  'domain': None,\n",
    "  'isProxy': False}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_data[0].get(\"securityContext\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "for idx, i in enumerate(okta_data):\n",
    "    if i.get(\"device\"):\n",
    "        print(idx)\n",
    "        n=idx\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_alternateId = okta_data[n].get(\"actor\").get(\"alternateId\")\n",
    "okta_displayname = okta_data[n].get(\"actor\").get(\"displayName\")\n",
    "okta_client = okta_data[n].get(\"client\")\n",
    "okta_event_publish_time = okta_data[n].get(\"published\")\n",
    "okta_transaction_data = okta_data[n].get(\"transaction\")\n",
    "okta_request_data = okta_data[n].get(\"request\")\n",
    "okta_security_context_data = okta_data[n].get(\"securityContext\")\n",
    "okta_device = okta_data[n].get(\"device\")\n",
    "okta_target = okta_data[n].get(\"target\")\n",
    "okta_data[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_data_list_df = pl.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"index\": idx,\n",
    "            \"alternateId\": x.get(\"actor\", {}).get(\"alternateId\", \"\"),\n",
    "            \"displayName\": x.get(\"actor\", {}).get(\"displayName\", \"\"),\n",
    "            \"device\": ({k: v for k, v in x.get(\"device\", {}).items() if k != \"id\"}\n",
    "                    if isinstance(x.get(\"device\", {}), dict)\n",
    "                    else {}),\n",
    "            \"client\": ({k: v for k, v in x.get(\"client\", {}).items() if k != \"id\"}\n",
    "                    if isinstance(x.get(\"client\", {}), dict)\n",
    "                    else {}),\n",
    "            \"securityContext\": ({k: v for k, v in x.get(\"securityContext\", {}).items() if k != \"id\"}\n",
    "                    if isinstance(x.get(\"securityContext\", {}), dict)\n",
    "                    else {}),\n",
    "            \"eventType\": x.get(\"eventType\", \"\"),\n",
    "            \"eventOutcome\": ({k: v for k, v in x.get(\"eventOutcome\", {}).items() if k != \"id\"}\n",
    "                    if isinstance(x.get(\"eventOutcome\", {}), dict)\n",
    "                    else {}),\n",
    "        }\n",
    "        for idx, x in enumerate(okta_data)\n",
    "        # if x.get(\"actor\", {}).get(\"type\") == \"User\"\n",
    "    ]\n",
    ")\n",
    "okta_data_list_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_schema = pl.Schema(\n",
    "        {\n",
    "            \"index\": pl.Int64,\n",
    "            \"alternateId\": pl.Int64,\n",
    "            \"displayName\": pl.String,\n",
    "            \"device\": pl.Struct(\n",
    "                {\n",
    "                    \"id\": pl.Utf8,\n",
    "                    \"name\": pl.Utf8,\n",
    "                    \"os_platform\": pl.Utf8,\n",
    "                    \"os_version\": pl.Utf8,\n",
    "                    \"managed\": pl.Boolean,\n",
    "                    \"registered\": pl.Boolean,\n",
    "                    \"device_integrator\": pl.Utf8,\n",
    "                    \"disk_encryption_type\": pl.Utf8,\n",
    "                    \"screen_lock_type\": pl.Utf8,\n",
    "                    \"jailbreak\": pl.Utf8,\n",
    "                    \"secure_hardware_present\": pl.Boolean,\n",
    "                }\n",
    "            ),\n",
    "            \"client\": pl.Struct(\n",
    "                {\n",
    "                    \"userAgent\": pl.Struct(\n",
    "                        {\n",
    "                            \"rawUserAgent\": pl.String,\n",
    "                            \"os\": pl.String,\n",
    "                            \"browser\": pl.String,\n",
    "                        }\n",
    "                    ),\n",
    "                    \"zone\": pl.String,\n",
    "                    \"device\": pl.String,\n",
    "                    \"id\": pl.String,\n",
    "                    \"ipAddress\": pl.String,\n",
    "                    \"geographicalContext\": pl.Struct(\n",
    "                        {\n",
    "                            \"city\": pl.String,\n",
    "                            \"state\": pl.String,\n",
    "                            \"country\": pl.String,\n",
    "                            \"postalCode\": pl.String,\n",
    "                            \"geolocation\": pl.Struct(\n",
    "                                {\n",
    "                                    \"lat\": pl.Float64,\n",
    "                                    \"lon\": pl.Float64,\n",
    "                                }\n",
    "                            ),\n",
    "                        }\n",
    "                    ),\n",
    "                }\n",
    "            ),\n",
    "            \"securityContext\": pl.Struct(\n",
    "                {\n",
    "                    \"securityContext.asNumber\": pl.Int64,\n",
    "                    \"securityContext.asOrg\": pl.Utf8,\n",
    "                    \"securityContext.isp\": pl.Utf8,\n",
    "                    \"domain\": pl.Utf8,\n",
    "                    \"isProxy\": pl.Boolean,\n",
    "                }\n",
    "            ),\n",
    "            \"eventType\": pl.String,\n",
    "            \"eventOutcome\": pl.Struct(\n",
    "                {\n",
    "                    \"result\": pl.Utf8,\n",
    "                    \"reason\": pl.Utf8,\n",
    "                }\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars_ds import str_fuzz, str_jw\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Replace null 'device' entries with an empty struct\n",
    "device_fields = [ \"device.name\", \"device.os_platform\", \"device.os_version\", \"device.managed\",\n",
    "                 \"device.registered\", \"device.device_integrator\", \"device.disk_encryption_type\",\n",
    "                 \"device.screen_lock_type\", \"device.jailbreak\", \"device.secure_hardware_present\"]\n",
    "client_fields = ['client.userAgent', 'client.zone', 'client.device', 'client.ipAddress', 'client.geographicalContext']\n",
    "securityContext_fields = ['securityContext.asNumber', 'securityContext.asOrg', 'isecurityContext.sp', 'securityContext.domain', 'securityContext.isProxy']\n",
    "eventOutcome_fields = ['eventOutcome.result', 'eventOutcome.reason']\n",
    "\n",
    "okta_data_list_df = okta_data_list_df.with_columns(\n",
    "    pl.col(\"alternateId\").str.strip_chars('\"').alias(\"alternateId\"),\n",
    "    pl.col(\"displayName\").str.strip_chars('\"').alias(\"displayName\"),\n",
    "    pl.when(pl.col(\"device\").is_null())\n",
    "      .then({field: None for field in device_fields})\n",
    "      .otherwise(pl.col(\"device\"))\n",
    "      .alias(\"device\"),\n",
    "    pl.when(pl.col(\"client\").is_null())\n",
    "      .then({field: None for field in client_fields})\n",
    "      .otherwise(pl.col(\"client\"))\n",
    "      .alias(\"client\"),\n",
    "    pl.when(pl.col(\"securityContext\").is_null())\n",
    "      .then({field: None for field in securityContext_fields})\n",
    "      .otherwise(pl.col(\"securityContext\"))\n",
    "      .alias(\"securityContext\"),\n",
    "    pl.when(pl.col(\"eventOutcome\").is_null())\n",
    "      .then({field: None for field in eventOutcome_fields})\n",
    "      .otherwise(pl.col(\"eventOutcome\"))\n",
    "      .alias(\"eventOutcome\"),\n",
    "# Add proxyAddresses list for emails\n",
    ")\n",
    "# okta_data_list_vec_df = okta_data_list_df.with_columns(\n",
    "#     okta_altIt_vec= pl.lit(model.encode(pl.col(\"alternateId\"), convert_to_tensor=True)).alias(\"okta_altId_vec\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_flat_df = (okta_data_list_df.unnest(\"device\")\n",
    "    .unnest(\"client\")\n",
    "    .unnest(\"securityContext\")\n",
    "    .unnest(\"eventOutcome\"))\n",
    "print(okta_flat_df.shape)\n",
    "print(okta_flat_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_flat_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "okta_data_list_vec_df = okta_data_list_df.with_columns(\n",
    "    pl.col(\"alternateId\").candle.embed_text(\"sentence-transformers/all-MiniLM-L6-v2\").alias(\"okta_altId_vec\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # username + email + device info + domain + security context + datetime (event publish time) + proxyAddresses\n",
    "n = 0\n",
    "for idx, i in enumerate(ad_users):\n",
    "    if i.get(\"devices\") and len(i.get(\"devices\")) > 0 and i.get(\"companyName\"):\n",
    "        # print(i)\n",
    "        n=idx\n",
    "        print(idx)\n",
    "        ad_display_name = i[\"displayName\"]\n",
    "        ad_mail_nickname = i[\"mailNickname\"]\n",
    "        ad_given_name = i[\"givenName\"]\n",
    "        ad_surname = i[\"surname\"]\n",
    "        ad_email = i[\"mail\"]\n",
    "        ad_department = i[\"department\"]\n",
    "        ad_usr_principal_name = i[\"userPrincipalName\"]\n",
    "        ad_onpremis_sam_acc_name = i[\"onPremisesSamAccountName\"]\n",
    "        ad_devices = i[\"devices\"]\n",
    "        ad_proxy_addresses = i[\"proxyAddresses\"]\n",
    "        ad_onpremis_distinguished_name = i[\"onPremisesDistinguishedName\"]\n",
    "        ad_company_name = i[\"companyName\"]\n",
    "        # ad_transitive_memberships = i[\"transitivelyMembership\"]\n",
    "        break\n",
    "# {'displayName': 'Kendell Douglas',\n",
    "#  'mailNickname': 'kdouglas1',\n",
    "#  'givenName': 'Kendell',\n",
    "#  'surname': 'Douglas',\n",
    "#  'jobTitle': 'Retail Planning Manager',\n",
    "#  'department': 'CLV_HEC: 08800_CES 9IF_CEI MERCHANDISING ADMIN',\n",
    "#  'mail': 'kdouglas1@caesars.com',\n",
    "#  'otherMails': ['kdouglas1@lvrio.harrahs.com'],\n",
    "# #  'proxyAddresses': ['X500:/O=Casino Windsor/OU=EXCHANGE/cn=Recipients/cn=kdouglas1',\n",
    "#  'city': 'Las Vegas',\n",
    "#  'state': 'NV',\n",
    "#  'country': 'United States',\n",
    "#  'streetAddress': '3570 S. Las Vegas Blvd',\n",
    "#  'userPrincipalName': 'kdouglas1@caesars.com',\n",
    "#  'onPremisesSamAccountName': 'kdouglas1',\n",
    "# transitiveMemberOf\n",
    "# manager@delta\n",
    "ad_users[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_with_most_keys(ad_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### String Matching\n",
    "\n",
    "Encode the Strings:\n",
    "- Pass your two strings to the model’s encode method to get their embeddings.\n",
    "- Calculate Similarity: Use cosine similarity to measure how similar the embeddings are (values near 1 indicate high similarity, near 0 indicate low similarity).\n",
    "- High cosine + high edit-based → genuine match (e.g., \"apple\" ~ \"appel\": cos≈0.8, lev≈91).\n",
    "- High cosine but low edit similarity → likely anagram or jumbled order (\"string\" vs \"gnirts\").\n",
    "- Low cosine but moderate Jaro-Winkler → prefix or truncation match (\"darth vader\" vs \"vader\": cos low, JW ≈ 0.9).\n",
    "- All low → clearly unrelated.\n",
    "\n",
    "Jaro-Winkler is very good at name matching due to shared prefixes.\n",
    "\n",
    "##### Minimums:\n",
    "- Jaro >= 80.0\n",
    "- Lev >= 75.0\n",
    "- vector_cosine\n",
    "\n",
    "Consider Damerau–Levenshtein for adjacent-swapped letters (e.g., \"form\" → \"from\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Levenshtein ratio (0–100) quantifies insertions, deletions, substitutions.\n",
    "# # Jaro‑Winkler (0–1) gives extra weight to common prefixes which helps with truncated or prefix‑similar strings\n",
    "#\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "#\n",
    "# from rapidfuzz import fuzz, distance  # well-maintained Levenshtein scoring\n",
    "# from jaro import jaro_winkler_metric\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# from sentence_transformers import util\n",
    "#\n",
    "# def compare_strings(s1: str, s2: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Compute multiple string-similarity metrics between s1 and s2.\n",
    "#\n",
    "#     Returns dict with:\n",
    "#       - levenshtein_ratio: normalized edit distance (0–100)\n",
    "#       - jaro_winkler: Jaro-Winkler similarity (0–1)\n",
    "#       - tfidf_cosine: cosine similarity on char‑ngram TF-IDF (0–1)\n",
    "#       - vector_cosine: vector cosine similarity using `sentence-transformers/all-MiniLM-L6-v2`\n",
    "#     \"\"\"\n",
    "#     # 1. Edit‐distance / Levenshtein Ratio (0–100)\n",
    "#     lev_ratio = fuzz.ratio(s1, s2)\n",
    "#\n",
    "#     # 2. Jaro‑Winkler (0–1)\n",
    "#     jw = jaro_winkler_metric(s1, s2)\n",
    "#\n",
    "#     # 3. Cosine similarity using TF‑IDF of char 3‑grams\n",
    "#     tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,3))\n",
    "#     tfidf_matrix = tfidf.fit_transform([s1, s2])\n",
    "#     cos = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0,0]\n",
    "#     # # Example strings to match\n",
    "#\n",
    "#     # Encode both strings\n",
    "#     embedding1 = model.encode(s1, convert_to_tensor=True)\n",
    "#     embedding2 = model.encode(s2, convert_to_tensor=True)\n",
    "#\n",
    "#     # Calculate cosine similarity\n",
    "#     vector_cosine_similarity = util.cos_sim(embedding1, embedding2)\n",
    "#     return {\n",
    "#         'levenshtein_ratio': lev_ratio,\n",
    "#         'jaro_winkler': jw,\n",
    "#         'tfidf_cosine': cos,\n",
    "#         'vector_cosine': vector_cosine_similarity.item()\n",
    "#     }\n",
    "#\n",
    "#\n",
    "# string1 = ad_display_name\n",
    "# # string2 = \"filomena@silva\"\n",
    "# # tests = [(string1, string2)]\n",
    "# okta_data_list = [{\"displayName\": x.get(\"displayName\", []),\"email\": x.get(\"main\", None), \"idx\": idx} for idx, x in enumerate(ad_users)]\n",
    "# for a in okta_data_list:\n",
    "#     string2 = a.get(\"displayName\")\n",
    "#     scores = compare_strings(string2, string1)\n",
    "#     # print(f\"{string2!r} ≈ {string1!r} →\", scores)\n",
    "#     lev, jaro, tfid, vec_cos = scores.values()\n",
    "#     if vec_cos >= 0.6:\n",
    "#         print(f\"Vector similarity score: {vec_cos:.4f}\")\n",
    "#         if float(lev) >= 75.0 and float(jaro) >= 85.0 and (vec_cos) >= 0.8:\n",
    "#             print(f\"{string2!r} ≈ {string1!r} →\", scores)\n",
    "#             print(\"Match!\")\n",
    "#             print(a)\n",
    "#             break\n",
    "#     if a.get(\"email\"):\n",
    "#         string2 = a.get(\"email\")\n",
    "#         scores = compare_strings(string2, string1)\n",
    "#         # print(f\"{string2!r} ≈ {string1!r} →\", scores)\n",
    "#         lev, jaro, tfid, vec_cos = scores.values()\n",
    "#         if vec_cos >= 0.6:\n",
    "#             print(f\"Vector similarity score: {vec_cos:.4f}\")\n",
    "#             if float(lev) >= 75.0 and float(jaro) >= 85.0 and (vec_cos) >= 0.8:\n",
    "#                 print(f\"{string2!r} ≈ {string1!r} →\", scores)\n",
    "#                 print(\"Match!\")\n",
    "#                 print(a)\n",
    "#                 break\n",
    "# ad_data_list_df = pl.from_records([{\"vector_displayName\": model.encode(x.get(\"displayName\", ''), convert_to_tensor=True),\n",
    "#                                       \"displayName\": x.get(\"displayName\", None),\"email\": x.get(\"mail\", None), \"idx\": idx} for idx, x in enumerate(ad_users[:1000])], schema=[\"idx\", \"displayName\", \"email\", \"vector_displayName\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select( # Column \"word\", compared to string in pl.lit(). It also supports column vs column comparison\n",
    "#     pds.str_leven(\"word\", pl.lit(\"asasasa\"), return_sim=True).alias(\"Levenshtein\"),\n",
    "#     pds.str_osa(\"word\", pl.lit(\"apples\"), return_sim=True).alias(\"Optimal String Alignment\"),\n",
    "#     pds.str_jw(\"word\", pl.lit(\"apples\")).alias(\"Jaro-Winkler\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://polars-ds-extension.readthedocs.io/en/latest/string.html\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "from sentence_transformers.util import cos_sim\n",
    "import polars as pl\n",
    "from polars_ds import str_fuzz, str_jw\n",
    "import polars_candle  # pip install polars-candle\n",
    "import numpy as np\n",
    "# Encode both strings\n",
    "t_string = 'kdouglas1@caesars.com'\n",
    "# Calculate cosine similarity\n",
    "vector_target = pl.lit(model.encode(t_string, convert_to_tensor=True).cpu().numpy().flatten(), dtype=pl.List(pl.Float64))\n",
    "target = pl.lit(t_string)\n",
    "sim_df = okta_flat_df.with_columns(\n",
    "    str_fuzz(pl.col(\"alternateId\"), target, parallel=True).alias(\"lev_sim\"),\n",
    "    str_jw(pl.col(\"alternateId\"), target, parallel=True).alias(\"jw_sim\"),\n",
    "    # vector_cos= pl.lit(cos_sim(pl.col(\"alternateId\")\n",
    "    #   .candle.embed_text(\"sentence-transformers/all-MiniLM-L6-v2\"), vector_target).item()\n",
    "    #   .alias(\"vector_cos\"), dtype=pl.Float64)\n",
    ")\n",
    "\n",
    "# df = df.with_columns([\n",
    "#     pl.col(\"displayName\")\n",
    "#       .candle.embed_text(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#       .alias(\"vector\"),\n",
    "#     pl.lit(\"filomena@silva\")\n",
    "#       .candle.embed_text(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#       .alias(\"target_vector\")\n",
    "# ])\n",
    "# df = df.with_columns(\n",
    "#     (pl.col(\"vector_displayName\").dot(pl.col(\"target_vector\"))\n",
    "#      / (pl.col(\"vector\").norm() * pl.col(\"target_vector\").norm()))\n",
    "#     .alias(\"vector_cosine\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.sort(by=\"jw_sim\", descending=True, nulls_last=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim_df.shape, sim_df.head(5).to_dict().get(\"alternateId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = sim_df.with_columns(\n",
    "#     contains=pl.col(\"alternateId\").is_in([t_string])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.filter(pl.col(\"alternateId\") == t_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "| Feature             | Levenshtein                                    | Jaro‑Winkler                                                                                        |   |    |                      |                                                 |\n",
    "| ------------------- | ---------------------------------------------- | --------------------------------------------------------------------------------------------------- | - | -- | -------------------- | ----------------------------------------------- |\n",
    "| **Edit operations** | insertion, deletion, substitution              | matching within window, transposition                                                               |   |    |                      |                                                 |\n",
    "| **Transposition**   | Not counted (unless using Damerau–Levenshtein) | Explicitly considered                                                                               |   |    |                      |                                                 |\n",
    "| **Prefix emphasis** | None                                           | Yes (up to 4 chars, via Winkler bonus) ([en.wikipedia.org][1], [dev.to][2], [geeksforgeeks.org][3]) |   |    |                      |                                                 |\n",
    "| **Range/Output**    | 0…∞ (distance), normalized 0–1 similarity      | 0–1 similarity                                                                                      |   |    |                      |                                                 |\n",
    "| **Best for**        | Longer strings, quantifying edit count         | Short-to-medium strings (names), with typos and prefix variations                                   |   |    |                      |                                                 |\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance?utm_source=chatgpt.com \"Jaro–Winkler distance\"\n",
    "[2]: https://dev.to/fatemasamir/unveiling-string-distances-a-dive-into-levenshtein-and-jaro-distances-in-databases-b6j?utm_source=chatgpt.com \"Unveiling String Distances: A Dive into Levenshtein and Jaro ...\"\n",
    "[3]: https://www.geeksforgeeks.org/jaro-and-jaro-winkler-similarity/?utm_source=chatgpt.com \"Jaro and Jaro-Winkler similarity - GeeksforGeeks\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "#### MS Exchange Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_data = None\n",
    "with open(\"../data/customers/caesars/2024-11-20_exchange.json\", 'r') as f:\n",
    "    exchange_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### TAP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This set of data is from the `/api/data/v1/people` API\n",
    "tap_users_data = None\n",
    "with open(\"../data/customers/caesars/2024-11-20_tap_users.json\", 'r') as f:\n",
    "    tap_users_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap_users_data[0:2]\n",
    "dict_with_most_keys(tap_users_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Azure AD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Azure AD Data\n",
    "azure_ad_data = None\n",
    "with open(\"../data/customers/caesars/2024-11-20_azure_ad.json\", 'r') as f:\n",
    "    azure_ad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_users = azure_ad_data.get(\"users\", [])\n",
    "# - Microsoft Entra group, a Microsoft 365 group, or a security group\n",
    "# - onPremisesSecurityIdentifier field is particularly important for hybrid environments where groups are synchronized between on-premises Active Directory and Azure AD, and enables correlation between cloud and on-premises representations of the same security principal.\n",
    "# - onPremisesSyncEnabled property indicates whether a group is synchronized from on-premises Active Directory,\n",
    "ad_groups = azure_ad_data.get(\"groups\", [])\n",
    "ad_devices = azure_ad_data.get(\"devices\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_user = dict_with_most_keys(ad_users)\n",
    "print(json.dumps(dict_with_most_keys(ad_users), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join AD user and group data by user devices\n",
    "for x in ad_groups:\n",
    "    if 'members@delta' in x.keys():\n",
    "        for n in x.get(\"members@delta\"):\n",
    "            if n.get(\"id\") == t_user.get(\"id\"):\n",
    "                print(x)\n",
    "            if n.get(\"id\") in [a.get(\"id\") for a in t_user.get(\"devices\")]:\n",
    "                print(f\"Found in device IDs:\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups[1]\n",
    "dict_with_most_keys(ad_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### BloodHound Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_users_data: list[dict] = []\n",
    "with open(\"../data/bloodhound/generated/ad_example_data/20240305110018_users.json\", 'r') as f:\n",
    "    bh_users_data.append(json.load(f))\n",
    "with open(\"../data/bloodhound/generated/ad_example_data/20240305110427_users.json\", 'r') as f:\n",
    "    bh_users_data.append(json.load(f))\n",
    "with open(\"../data/bloodhound/generated/ad_example_data/20240305111414_users.json\", 'r') as f:\n",
    "    bh_users_data.append(json.load(f))\n",
    "print(len(bh_users_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bh_users_data[0].get(\"data\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_users_data[0].get(\"data\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_computers_data: list[dict] = []\n",
    "\n",
    "with open(\"../data/bloodhound/generated/ad_example_data/20240305110018_computers.json\", 'r') as f:\n",
    "    bh_computers_data.append(json.load(f))\n",
    "with open(\"../data/bloodhound/generated/ad_example_data/20240305110427_computers.json\", 'r') as f:\n",
    "    bh_computers_data.append(json.load(f))\n",
    "with open(\"../data/bloodhound/generated/ad_example_data/20240305111414_computers.json\", 'r') as f:\n",
    "    bh_computers_data.append(json.load(f))\n",
    "print(len(bh_computers_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bh_computers_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_computers_data[0].get(\"data\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### AD transforms to Bloodhound Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.11\n",
    "\"\"\"\n",
    "Microsoft Graph API to BloodHound CE Group Mapper\n",
    "Transforms Microsoft Graph API v1 group objects to BloodHound CE format\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import re\n",
    "\n",
    "\n",
    "class GraphToBloodHoundMapper:\n",
    "    \"\"\"Maps Microsoft Graph API group data to BloodHound CE format\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.domain_suffix = \"\"\n",
    "        self.collected_timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "    def set_domain_suffix(self, domain: str):\n",
    "        \"\"\"Set the domain suffix for group objects\"\"\"\n",
    "        self.domain_suffix = domain.upper()\n",
    "\n",
    "    def extract_domain_from_upn(self, upn: str) -> str:\n",
    "        \"\"\"Extract domain from user principal name or email\"\"\"\n",
    "        if \"@\" in upn:\n",
    "            return upn.split(\"@\")[1].upper()\n",
    "        return self.domain_suffix\n",
    "\n",
    "    def generate_distinguished_name(self, group_name: str, domain: str) -> str:\n",
    "        \"\"\"Generate a distinguished name for the group\"\"\"\n",
    "        # Simple DN generation - in practice, you might want to get actual DN from Graph API\n",
    "        domain_components = \".\".join([f\"DC={part}\" for part in domain.lower().split(\".\")])\n",
    "        return f\"CN={group_name},CN=Users,{domain_components}\"\n",
    "\n",
    "    def map_group_to_bloodhound(self, graph_group: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Map a Microsoft Graph API group object to BloodHound CE format\n",
    "\n",
    "        Args:\n",
    "            graph_group: Group object from Microsoft Graph API\n",
    "\n",
    "        Returns:\n",
    "            Dictionary in BloodHound CE group format\n",
    "        \"\"\"\n",
    "        # Extract basic properties\n",
    "        object_id = graph_group.get(\"id\", \"\")\n",
    "        display_name = graph_group.get(\"displayName\", \"\")\n",
    "        mail = graph_group.get(\"mail\", \"\")\n",
    "        description = graph_group.get(\"description\", \"\")\n",
    "\n",
    "        # Determine domain\n",
    "        domain = self.domain_suffix\n",
    "        if mail and \"@\" in mail:\n",
    "            domain = self.extract_domain_from_upn(mail)\n",
    "        elif graph_group.get(\"onPremisesDomainName\"):\n",
    "            domain = graph_group[\"onPremisesDomainName\"].upper()\n",
    "\n",
    "        # Security properties\n",
    "        security_enabled = graph_group.get(\"securityEnabled\", False)\n",
    "        security_identifier = graph_group.get(\"securityIdentifier\", \"\")\n",
    "        on_premises_sid = graph_group.get(\"onPremisesSecurityIdentifier\", \"\")\n",
    "        is_assignable_to_role = graph_group.get(\"isAssignableToRole\", False)\n",
    "\n",
    "        # Use on-premises SID if available, otherwise use Azure AD SID\n",
    "        primary_sid = on_premises_sid if on_premises_sid else security_identifier\n",
    "\n",
    "        # Group types\n",
    "        group_types = graph_group.get(\"groupTypes\", [])\n",
    "        is_unified_group = \"Unified\" in group_types\n",
    "\n",
    "        # Mail properties\n",
    "        mail_enabled = graph_group.get(\"mailEnabled\", False)\n",
    "        mail_nickname = graph_group.get(\"mailNickname\", \"\")\n",
    "\n",
    "        # Generate BloodHound group object\n",
    "        bloodhound_group = {\n",
    "            \"ObjectIdentifier\": object_id.upper(),\n",
    "            \"Name\": f\"{display_name}@{domain}\",\n",
    "            \"Domain\": domain,\n",
    "            \"DistinguishedName\": self.generate_distinguished_name(display_name, domain),\n",
    "            \"Description\": description,\n",
    "            # \"AdminCount\": False,  # Would need additional logic to determine\n",
    "            \"Properties\": {\n",
    "                \"name\": display_name,\n",
    "                \"domain\": domain,\n",
    "                \"objectid\": object_id.upper(),\n",
    "                \"description\": description,\n",
    "                \"distinguishedname\": self.generate_distinguished_name(display_name, domain),\n",
    "                \"domainsid\": self._extract_domain_sid(primary_sid) if primary_sid else \"\",\n",
    "                \"securityenabled\": security_enabled,\n",
    "                \"mailnickname\": mail_nickname,\n",
    "                \"mailenabled\": mail_enabled,\n",
    "                \"mail\": mail or \"\",\n",
    "                \"isassignabletorole\": is_assignable_to_role,\n",
    "                \"grouptype\": self._map_group_type(group_types, security_enabled),\n",
    "                \"onpremisessecurityidentifier\": on_premises_sid,\n",
    "                \"azuread_objectid\": object_id,\n",
    "                \"collected\": True,\n",
    "                \"crossref\": None\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add optional properties if they exist\n",
    "        if graph_group.get(\"createdDateTime\"):\n",
    "            bloodhound_group[\"Properties\"][\"whencreated\"] = graph_group[\"createdDateTime\"]\n",
    "\n",
    "        if graph_group.get(\"onPremisesLastSyncDateTime\"):\n",
    "            bloodhound_group[\"Properties\"][\"lastlogon\"] = graph_group[\"onPremisesLastSyncDateTime\"]\n",
    "\n",
    "        return bloodhound_group\n",
    "\n",
    "    def _extract_domain_sid(self, sid: str) -> str:\n",
    "        \"\"\"Extract domain SID from full SID\"\"\"\n",
    "        if not sid:\n",
    "            return \"\"\n",
    "\n",
    "        # SID format: S-1-5-21-domain-domain-domain-rid\n",
    "        # We want everything except the last part (RID)\n",
    "        parts = sid.split(\"-\")\n",
    "        if len(parts) >= 4:\n",
    "            return \"-\".join(parts[:-1])\n",
    "        return \"\"\n",
    "\n",
    "    def _map_group_type(self, group_types: List[str], security_enabled: bool) -> int:\n",
    "        \"\"\"Map Graph API group types to BloodHound group type integer\"\"\"\n",
    "        # BloodHound group types (simplified mapping)\n",
    "        if security_enabled:\n",
    "            if \"Unified\" in group_types:\n",
    "                return 0x80000000  # Universal security group with mail\n",
    "            else:\n",
    "                return 0x80000002  # Global security group\n",
    "        else:\n",
    "            if \"Unified\" in group_types:\n",
    "                return 0x00000000  # Universal distribution group\n",
    "            else:\n",
    "                return 0x00000004  # Global distribution group\n",
    "\n",
    "    def map_group_members(self, graph_group: Dict[str, Any], members_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Map group membership relationships to BloodHound format\n",
    "\n",
    "        Args:\n",
    "            graph_group: The parent group object\n",
    "            members_data: List of member objects from Graph API\n",
    "\n",
    "        Returns:\n",
    "            List of membership relationship objects\n",
    "        \"\"\"\n",
    "        relationships = []\n",
    "        group_id = graph_group.get(\"id\", \"\").upper()\n",
    "        domain = self.domain_suffix\n",
    "\n",
    "        if graph_group.get(\"mail\") and \"@\" in graph_group[\"mail\"]:\n",
    "            domain = self.extract_domain_from_upn(graph_group[\"mail\"])\n",
    "        elif graph_group.get(\"onPremisesDomainName\"):\n",
    "            domain = graph_group[\"onPremisesDomainName\"].upper()\n",
    "\n",
    "        for member in members_data:\n",
    "            member_id = member.get(\"id\", \"\").upper()\n",
    "            member_type = member.get(\"@odata.type\", \"\")\n",
    "\n",
    "            # Determine member type for BloodHound\n",
    "            if \"user\" in member_type.lower():\n",
    "                member_label = \"User\"\n",
    "            elif \"group\" in member_type.lower():\n",
    "                member_label = \"Group\"\n",
    "            elif \"device\" in member_type.lower():\n",
    "                member_label = \"Computer\"\n",
    "            else:\n",
    "                member_label = \"Base\"  # Default\n",
    "\n",
    "            relationship = {\n",
    "                \"SourceObjectIdentifier\": member_id,\n",
    "                \"SourceObjectType\": member_label,\n",
    "                \"TargetObjectIdentifier\": group_id,\n",
    "                \"TargetObjectType\": \"Group\",\n",
    "                \"RelationshipType\": \"MemberOf\"\n",
    "            }\n",
    "\n",
    "            relationships.append(relationship)\n",
    "\n",
    "        return relationships\n",
    "\n",
    "    def create_bloodhound_data_structure(self, groups: List[Dict[str, Any]],\n",
    "                                       membership_data: Dict[str, List[Dict[str, Any]]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create complete BloodHound data structure for ingestion\n",
    "\n",
    "        Args:\n",
    "            groups: List of Graph API group objects\n",
    "            membership_data: Optional dictionary mapping group IDs to member lists\n",
    "\n",
    "        Returns:\n",
    "            Complete BloodHound data structure\n",
    "        \"\"\"\n",
    "        bloodhound_groups = []\n",
    "        relationships = []\n",
    "\n",
    "        for group in groups:\n",
    "            # Map the group\n",
    "            bh_group = self.map_group_to_bloodhound(group)\n",
    "            bloodhound_groups.append(bh_group)\n",
    "\n",
    "            # Map memberships if provided\n",
    "            if membership_data and group.get(\"id\") in membership_data:\n",
    "                group_relationships = self.map_group_members(group, membership_data[group[\"id\"]])\n",
    "                relationships.extend(group_relationships)\n",
    "\n",
    "        # Create BloodHound data structure\n",
    "        bloodhound_data = {\n",
    "            \"data\": bloodhound_groups,\n",
    "            \"meta\": {\n",
    "                \"type\": \"groups\",\n",
    "                \"count\": len(bloodhound_groups),\n",
    "                \"version\": 6,\n",
    "                \"collected_by\": \"graph-api-mapper\",\n",
    "                \"collected_time\": self.collected_timestamp\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add relationships if any exist\n",
    "        if relationships:\n",
    "            bloodhound_data[\"relationships\"] = relationships\n",
    "\n",
    "        return bloodhound_data\n",
    "\n",
    "\n",
    "mapper = GraphToBloodHoundMapper()\n",
    "mapper.set_domain_suffix(\"CAESARS.COM\")\n",
    "\n",
    "bloodhound_data = mapper.create_bloodhound_data_structure(ad_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(bloodhound_data[\"data\"][0], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.11\n",
    "\"\"\"\n",
    "Microsoft Graph API to BloodHound CE User Mapper\n",
    "Transforms Microsoft Graph API v1 user objects to BloodHound CE format\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import re\n",
    "\n",
    "\n",
    "class GraphUserToBloodHoundMapper:\n",
    "    \"\"\"Maps Microsoft Graph API user data to BloodHound CE format\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.domain_suffix = \"\"\n",
    "        self.collected_timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "    def set_domain_suffix(self, domain: str):\n",
    "        \"\"\"Set the domain suffix for user objects\"\"\"\n",
    "        self.domain_suffix = domain.upper()\n",
    "\n",
    "    def extract_domain_from_upn(self, upn: str) -> str:\n",
    "        \"\"\"Extract domain from user principal name or email\"\"\"\n",
    "        if \"@\" in upn:\n",
    "            return upn.split(\"@\")[1].upper()\n",
    "        return self.domain_suffix\n",
    "\n",
    "    def generate_distinguished_name(self, user_name: str, domain: str) -> str:\n",
    "        \"\"\"Generate a distinguished name for the user\"\"\"\n",
    "        domain_components = \".\".join([f\"DC={part}\" for part in domain.lower().split(\".\")])\n",
    "        return f\"CN={user_name},CN=Users,{domain_components}\"\n",
    "\n",
    "    def map_user_to_bloodhound(self, graph_user: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Map a Microsoft Graph API user object to BloodHound CE format\n",
    "\n",
    "        Args:\n",
    "            graph_user: User object from Microsoft Graph API\n",
    "\n",
    "        Returns:\n",
    "            Dictionary in BloodHound CE user format\n",
    "        \"\"\"\n",
    "        # Extract core properties\n",
    "        object_id = graph_user.get(\"id\", \"\")\n",
    "        user_principal_name = graph_user.get(\"userPrincipalName\", \"\")\n",
    "        display_name = graph_user.get(\"displayName\", \"\")\n",
    "        mail = graph_user.get(\"mail\", \"\")\n",
    "        description = graph_user.get(\"description\", \"\")\n",
    "\n",
    "        # Determine domain\n",
    "        domain = self.domain_suffix\n",
    "        if user_principal_name:\n",
    "            domain = self.extract_domain_from_upn(user_principal_name)\n",
    "        elif graph_user.get(\"onPremisesDomainName\"):\n",
    "            domain = graph_user[\"onPremisesDomainName\"].upper()\n",
    "\n",
    "        # Security identifiers\n",
    "        on_premises_sid = graph_user.get(\"onPremisesSecurityIdentifier\", \"\")\n",
    "        cloud_sid = graph_user.get(\"securityIdentifier\", \"\")\n",
    "        primary_sid = on_premises_sid if on_premises_sid else cloud_sid\n",
    "\n",
    "        # Account status and password properties\n",
    "        account_enabled = graph_user.get(\"accountEnabled\", False)\n",
    "        password_policies = graph_user.get(\"passwordPolicies\", \"\")\n",
    "        last_password_change = graph_user.get(\"lastPasswordChangeDateTime\", \"\")\n",
    "\n",
    "        # Generate BloodHound user object\n",
    "        bloodhound_user = {\n",
    "            \"ObjectIdentifier\": primary_sid.upper() if primary_sid else object_id.upper(),\n",
    "            \"PrimaryGroupSid\": self._get_primary_group_sid(primary_sid),\n",
    "            \"Properties\": {\n",
    "                \"name\": user_principal_name if user_principal_name else display_name,\n",
    "                \"domain\": domain,\n",
    "                \"distinguishedname\": graph_user.get(\"onPremisesDistinguishedName\")\n",
    "                                   or self.generate_distinguished_name(display_name, domain),\n",
    "                \"domainsid\": self._extract_domain_sid(primary_sid) if primary_sid else \"\",\n",
    "                \"enabled\": account_enabled,\n",
    "                \"email\": mail or \"\",\n",
    "                \"title\": graph_user.get(\"jobTitle\", \"\"),\n",
    "                \"homedirectory\": \"\",\n",
    "                \"scriptpath\": \"\",\n",
    "                \"profilepath\": \"\",\n",
    "                \"userpassword\": \"\",\n",
    "                \"admincount\": False,  # Requires additional logic to determine\n",
    "                \"sidhistory\": [],\n",
    "                \"passwordnotreqd\": \"DisablePasswordExpiration\" in password_policies,\n",
    "                \"pwdlastset\": last_password_change,\n",
    "                \"lastlogon\": graph_user.get(\"signInActivity\", {}).get(\"lastSignInDateTime\", \"\"),\n",
    "                \"whencreated\": graph_user.get(\"createdDateTime\", \"\"),\n",
    "                \"badpwdcount\": 0,  # Not available in Graph API\n",
    "                \"userprincipalname\": user_principal_name,\n",
    "                \"serviceprincipalnames\": [],\n",
    "                \"displayname\": display_name,\n",
    "                \"upndomain\": domain,\n",
    "                \"description\": description,\n",
    "                \"trustedtoauth\": False,\n",
    "                \"hasspn\": False,\n",
    "                \"sensitive\": False,\n",
    "                \"dontreqpreauth\": False,\n",
    "                \"passwordexpired\": \"DisablePasswordExpiration\" not in password_policies,\n",
    "                \"unconstraineddelegation\": False,\n",
    "                \"reversiblepassword\": False,\n",
    "                \"trustedfordelegation\": False,\n",
    "                \"collected\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add optional properties\n",
    "        if graph_user.get(\"employeeId\"):\n",
    "            bloodhound_user[\"Properties\"][\"employeeid\"] = graph_user[\"employeeId\"]\n",
    "\n",
    "        if graph_user.get(\"department\"):\n",
    "            bloodhound_user[\"Properties\"][\"department\"] = graph_user[\"department\"]\n",
    "\n",
    "        return bloodhound_user\n",
    "\n",
    "    def _get_primary_group_sid(self, user_sid: str) -> str:\n",
    "        \"\"\"Derive primary group SID (typically Domain Users)\"\"\"\n",
    "        if not user_sid:\n",
    "            return \"\"\n",
    "\n",
    "        # Extract domain SID and append Domain Users RID (513)\n",
    "        parts = user_sid.split(\"-\")\n",
    "        if len(parts) > 5:\n",
    "            domain_sid = \"-\".join(parts[:-1])\n",
    "            return f\"{domain_sid}-513\"\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_domain_sid(self, sid: str) -> str:\n",
    "        \"\"\"Extract domain SID from full user SID\"\"\"\n",
    "        if not sid:\n",
    "            return \"\"\n",
    "\n",
    "        parts = sid.split(\"-\")\n",
    "        if len(parts) >= 4:\n",
    "            return \"-\".join(parts[:-1])\n",
    "        return \"\"\n",
    "\n",
    "    def map_user_relationships(self, graph_user: Dict[str, Any],\n",
    "                             member_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Map user relationships to BloodHound format\n",
    "\n",
    "        Args:\n",
    "            graph_user: The user object\n",
    "            member_data: Dictionary containing memberOf, directReports, etc.\n",
    "\n",
    "        Returns:\n",
    "            List of relationship objects\n",
    "        \"\"\"\n",
    "        relationships = []\n",
    "        user_sid = graph_user.get(\"onPremisesSecurityIdentifier\", \"\") or graph_user.get(\"id\", \"\")\n",
    "\n",
    "        # Map group memberships\n",
    "        for group in member_data.get(\"memberOf\", []):\n",
    "            relationships.append({\n",
    "                \"SourceObjectIdentifier\": user_sid,\n",
    "                \"SourceObjectType\": \"User\",\n",
    "                \"TargetObjectIdentifier\": group.get(\"id\", \"\"),\n",
    "                \"TargetObjectType\": \"Group\",\n",
    "                \"RelationshipType\": \"MemberOf\"\n",
    "            })\n",
    "\n",
    "        # Map manager relationship\n",
    "        if graph_user.get(\"manager\", {}).get(\"id\"):\n",
    "            relationships.append({\n",
    "                \"SourceObjectIdentifier\": user_sid,\n",
    "                \"SourceObjectType\": \"User\",\n",
    "                \"TargetObjectIdentifier\": graph_user[\"manager\"][\"id\"],\n",
    "                \"TargetObjectType\": \"User\",\n",
    "                \"RelationshipType\": \"ReportsTo\"\n",
    "            })\n",
    "\n",
    "        # Map owned objects\n",
    "        for obj in member_data.get(\"ownedObjects\", []):\n",
    "            relationships.append({\n",
    "                \"SourceObjectIdentifier\": user_sid,\n",
    "                \"SourceObjectType\": \"User\",\n",
    "                \"TargetObjectIdentifier\": obj.get(\"id\", \"\"),\n",
    "                \"TargetObjectType\": self._get_object_type(obj),\n",
    "                \"RelationshipType\": \"Owns\"\n",
    "            })\n",
    "\n",
    "        return relationships\n",
    "\n",
    "    def _get_object_type(self, directory_object: Dict[str, Any]) -> str:\n",
    "        \"\"\"Determine BloodHound object type from Graph API @odata.type\"\"\"\n",
    "        type_map = {\n",
    "            \"#microsoft.graph.user\": \"User\",\n",
    "            \"#microsoft.graph.group\": \"Group\",\n",
    "            \"#microsoft.graph.device\": \"Computer\",\n",
    "            \"#microsoft.graph.application\": \"ServicePrincipal\"\n",
    "        }\n",
    "        return type_map.get(directory_object.get(\"@odata.type\", \"\"), \"Base\")\n",
    "\n",
    "\n",
    "class BloodHoundUserIngestor:\n",
    "    \"\"\"Handles complete user data ingestion workflow\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mapper = GraphUserToBloodHoundMapper()\n",
    "\n",
    "    def process_users(self, graph_users: List[Dict[str, Any]],\n",
    "                    relationship_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process complete user dataset with relationships\n",
    "\n",
    "        Args:\n",
    "            graph_users: List of Graph API user objects\n",
    "            relationship_data: Dictionary of user IDs to relationship data\n",
    "\n",
    "        Returns:\n",
    "            BloodHound CE compatible JSON structure\n",
    "        \"\"\"\n",
    "        bloodhound_users = []\n",
    "        relationships = []\n",
    "\n",
    "        for user in graph_users:\n",
    "            # Map user object\n",
    "            bh_user = self.mapper.map_user_to_bloodhound(user)\n",
    "            bloodhound_users.append(bh_user)\n",
    "\n",
    "            # Map relationships\n",
    "            user_id = user.get(\"id\", \"\")\n",
    "            if user_id in relationship_data:\n",
    "                user_rels = self.mapper.map_user_relationships(user, relationship_data[user_id])\n",
    "                relationships.extend(user_rels)\n",
    "\n",
    "        return {\n",
    "            \"data\": bloodhound_users,\n",
    "            \"meta\": {\n",
    "                \"type\": \"users\",\n",
    "                \"count\": len(bloodhound_users),\n",
    "                \"version\": 6,\n",
    "                \"collected_by\": \"graph-api-user-mapper\",\n",
    "                \"collected_time\": self.mapper.collected_timestamp\n",
    "            },\n",
    "            \"relationships\": relationships\n",
    "        }\n",
    "\n",
    "    # Sample relationship data\n",
    "    sample_relationships = {\n",
    "        \"a1b2c3d4-e5f6-7g8h-9i0j-k1l2m3n4o5p6\": {\n",
    "            \"memberOf\": [{\"id\": \"g1g2g3g4-h5h6-i7i8-j9j0-k1k2k3k4k5k6\", \"@odata.type\": \"#microsoft.graph.group\"}],\n",
    "            \"manager\": {\"id\": \"m1m2m3m4-n5n6-o7o8-p9p0-q1q2q3q4q5q6\", \"@odata.type\": \"#microsoft.graph.user\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Process data\n",
    "    ingestor = BloodHoundUserIngestor()\n",
    "    ingestor.mapper.set_domain_suffix(\"CAESARS.COM\")\n",
    "    bh_data = ingestor.process_users([sample_user], sample_relationships)\n",
    "\n",
    "    # Output result\n",
    "    print(json.dumps(bh_data, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
